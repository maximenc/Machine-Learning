{"cells":[{"cell_type":"markdown","metadata":{"id":"DIGMtvzfRSqB"},"source":["#### **1. Tokenizers**"]},{"cell_type":"markdown","metadata":{"id":"IMzuLRV3Q84Z"},"source":["Each tokenizer processes the text differently, reflecting its specific approach to handling words, punctuation, and sentence boundaries. The choice of tokenizer depends on the requirements of your NLP task and the nature of your text data\n","\n","### White Space Tokenization:\n","- **Splits text based on spaces.**\n","- **Expected Tokens:** `[\"Let's\", \"see\", \"different\", \"tokenizers.\", \"They\", \"all\", \"work\", \"differently:\", \"white-space,\", \"regex,\", \"NLTK,\", \"Spacy!\"]`\n","\n","### NLTK Word Tokenization:\n","- **Deals with punctuation and contractions.**\n","- **Expected Tokens:** `[\"Let\", \"'s\", \"see\", \"different\", \"tokenizers\", \".\", \"They\", \"all\", \"work\", \"differently\", \":\", \"white-space\", \",\", \"regex\", \",\", \"NLTK\", \",\", \"Spacy\", \"!\"]`\n","\n","### Regular Expression Tokenizer:\n","- **Uses a regex pattern (here \\w+ which matches any word character).**\n","- **Expected Tokens:** `[\"Let\", \"s\", \"see\", \"different\", \"tokenizers\", \"They\", \"all\", \"work\", \"differently\", \"white\", \"space\", \"regex\", \"NLTK\", \"Spacy\"]`\n","\n","### Spacy Tokenization:\n","- **Sophisticated tokenizer handling punctuation, special characters.**\n","- **Expected Tokens:** `[\"Let\", \"'s\", \"see\", \"different\", \"tokenizers\", \".\", \"They\", \"all\", \"work\", \"differently\", \":\", \"white-space\", \",\", \"regex\", \",\", \"NLTK\", \",\", \"Spacy\", \"!\"]`\n","\n","### Sentence Tokenization:\n","- **Splits text into sentences.**\n","- **Expected Sentences:** `[\"Let's see different tokenizers.\", \"They all work differently: white-space, regex, NLTK, Spacy!\"]`\n","\n","Each tokenizer processes the text differently, reflecting its specific approach to handling words, punctuation, and sentence boundaries. The choice of tokenizer depends on the requirements of your NLP task and the nature of your text data."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13206,"status":"ok","timestamp":1700761284351,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"},"user_tz":-60},"id":"iYk_kz7bQ5Dy","outputId":"754f11f2-fa72-41bf-c57d-7be45a699e33"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/maximen/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["White Space Tokenization\n","['The', 'quick,', 'brown', 'fox', '(an', 'exemplary', 'species)', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","NLTK Tokenization\n","['The', 'quick', ',', 'brown', 'fox', '(', 'an', 'exemplary', 'species', ')', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Regex Tokenization\n","['The', 'quick', 'brown', 'fox', 'an', 'exemplary', 'species', 'jumps', 'over', 'the', 'lazy', 'dog', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Spacy Tokenization\n","['The', 'quick', ',', 'brown', 'fox', '(', 'an', 'exemplary', 'species', ')', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Sentence Tokenization\n","['The quick, brown fox (an exemplary species) jumps over the lazy dog - a classic example of a pangram']\n"]}],"source":["import nltk\n","import spacy\n","from nltk.tokenize import word_tokenize, RegexpTokenizer, sent_tokenize\n","\n","# Sample text\n","text = \"The quick, brown fox (an exemplary species) jumps over the lazy dog - a classic example of a pangram\"\n","\n","# White Space Tokenization\n","white_space_tokens = text.split()\n","\n","# NLTK Word Tokenization\n","nltk.download('punkt')\n","nltk_tokens = word_tokenize(text)\n","\n","# Regular Expression Tokenizer\n","regexp_tokenizer = RegexpTokenizer(r'\\w+')\n","regex_tokens = regexp_tokenizer.tokenize(text)\n","\n","# Spacy Tokenization\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","spacy_tokens = [token.text for token in doc]\n","\n","# Sentence Tokenization\n","sentences = sent_tokenize(text)\n","\n","# Collecting all tokenization results\n","tokenization_results = {\n","    \"White Space Tokenization\": white_space_tokens,\n","    \"NLTK Tokenization\": nltk_tokens,\n","    \"Regex Tokenization\": regex_tokens,\n","    \"Spacy Tokenization\": spacy_tokens,\n","    \"Sentence Tokenization\": sentences\n","}\n","\n","for k in tokenization_results.keys():\n","  print(k)\n","  print(tokenization_results[k])"]},{"cell_type":"markdown","metadata":{},"source":["#### **2. Stemming**"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Sentence:\n"," The last meeting was exhausting. I am meeting my mom\n","\n","Stemmed Words:\n"," the last meet wa exhaust . i am meet my mom\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     /Users/maximen/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import nltk \n","nltk.download('punkt_tab')\n","\n","sentence = \"\"\"The last meeting was exhausting. I am meeting my mom\"\"\"\n","\n","# Initialize the PorterStemmer\n","stemmer = nltk.stem.PorterStemmer()\n","\n","# Tokenize the sentence into words\n","words = nltk.tokenize.word_tokenize(sentence)\n","\n","# Apply stemming to each word\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","print(\"Original Sentence:\\n\", sentence)\n","print(\"\\nStemmed Words:\\n\", \" \".join(stemmed_words))"]},{"cell_type":"markdown","metadata":{},"source":["#### **3. Lemmatization**"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /Users/maximen/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /Users/maximen/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Original Sentence:\n"," The last meeting was exhausting. I am meeting my mom\n","\n","Stemmed Words:\n"," The last meeting wa exhausting . I am meeting my mom\n"]}],"source":["import nltk\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')  # Optional for extended WordNet data\n","\n","lemmatizer = nltk.stem.WordNetLemmatizer() # Initialize the WordNetLemmatizer\n","\n","sentence = \"\"\"The last meeting was exhausting. I am meeting my mom\"\"\"\n","\n","# Tokenize the sentence into words\n","words = nltk.tokenize.word_tokenize(sentence)\n","\n","# Apply lemmatization to each word\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","print(\"Original Sentence:\\n\", sentence)\n","print(\"\\nStemmed Words:\\n\", \" \".join(lemmatized_words))"]},{"cell_type":"markdown","metadata":{},"source":["#### **4. POS tagging**"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Word and POS Tags:\n"," [('The', 'DT'), ('last', 'JJ'), ('meeting', 'NN'), ('was', 'VBD'), ('exhausting', 'VBG'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('meeting', 'VBG'), ('my', 'PRP$'), ('mom', 'NN')]\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/maximen/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /Users/maximen/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import nltk\n","\n","# Download necessary data for POS tagging\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","sentence = \"\"\"The last meeting was exhausting. I am meeting my mom\"\"\"\n","\n","# Tokenize the sentence into words\n","words = nltk.word_tokenize(sentence)\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(words)\n","\n","print(\"Word and POS Tags:\\n\", pos_tags)"]},{"cell_type":"markdown","metadata":{},"source":["#### **6. NER**"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Named Entities:\n"," (S\n","  (ORGANIZATION AbbVie/NNP Inc./NNP)\n","  is/VBZ\n","  an/DT\n","  (GPE American/JJ)\n","  pharmaceutical/JJ\n","  company/NN\n","  headquartered/VBD\n","  in/IN\n","  (GPE North/NNP Chicago/NNP)\n","  ,/,\n","  (GPE Illinois/NNP)\n","  ./.)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/maximen/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /Users/maximen/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n","[nltk_data] Downloading package words to /Users/maximen/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}],"source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('maxent_ne_chunker_tab')\n","nltk.download('words')\n","\n","sentence = \"AbbVie Inc. is an American pharmaceutical company headquartered in North Chicago, Illinois.\"\n","\n","# Tokenize the sentence into words\n","words = nltk.word_tokenize(sentence)\n","\n","# Tag the words with part-of-speech\n","pos_tags = nltk.pos_tag(words)\n","\n","# Perform Named Entity Recognition\n","named_entities = nltk.ne_chunk(pos_tags)\n","\n","print(\"Named Entities:\\n\", named_entities)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMkQKrIG62RL8yIDzbNyuQz","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
