{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkQKrIG62RL8yIDzbNyuQz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Different Tokenizers"],"metadata":{"id":"DIGMtvzfRSqB"}},{"cell_type":"markdown","source":["Each tokenizer processes the text differently, reflecting its specific approach to handling words, punctuation, and sentence boundaries. The choice of tokenizer depends on the requirements of your NLP task and the nature of your text data\n","\n","### White Space Tokenization:\n","- **Splits text based on spaces.**\n","- **Expected Tokens:** `[\"Let's\", \"see\", \"different\", \"tokenizers.\", \"They\", \"all\", \"work\", \"differently:\", \"white-space,\", \"regex,\", \"NLTK,\", \"Spacy!\"]`\n","\n","### NLTK Word Tokenization:\n","- **Deals with punctuation and contractions.**\n","- **Expected Tokens:** `[\"Let\", \"'s\", \"see\", \"different\", \"tokenizers\", \".\", \"They\", \"all\", \"work\", \"differently\", \":\", \"white-space\", \",\", \"regex\", \",\", \"NLTK\", \",\", \"Spacy\", \"!\"]`\n","\n","### Regular Expression Tokenizer:\n","- **Uses a regex pattern (here \\w+ which matches any word character).**\n","- **Expected Tokens:** `[\"Let\", \"s\", \"see\", \"different\", \"tokenizers\", \"They\", \"all\", \"work\", \"differently\", \"white\", \"space\", \"regex\", \"NLTK\", \"Spacy\"]`\n","\n","### Spacy Tokenization:\n","- **Sophisticated tokenizer handling punctuation, special characters.**\n","- **Expected Tokens:** `[\"Let\", \"'s\", \"see\", \"different\", \"tokenizers\", \".\", \"They\", \"all\", \"work\", \"differently\", \":\", \"white-space\", \",\", \"regex\", \",\", \"NLTK\", \",\", \"Spacy\", \"!\"]`\n","\n","### Sentence Tokenization:\n","- **Splits text into sentences.**\n","- **Expected Sentences:** `[\"Let's see different tokenizers.\", \"They all work differently: white-space, regex, NLTK, Spacy!\"]`\n","\n","Each tokenizer processes the text differently, reflecting its specific approach to handling words, punctuation, and sentence boundaries. The choice of tokenizer depends on the requirements of your NLP task and the nature of your text data."],"metadata":{"id":"IMzuLRV3Q84Z"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYk_kz7bQ5Dy","executionInfo":{"status":"ok","timestamp":1700761284351,"user_tz":-60,"elapsed":13206,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"754f11f2-fa72-41bf-c57d-7be45a699e33"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["White Space Tokenization\n","['The', 'quick,', 'brown', 'fox', '(an', 'exemplary', 'species)', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","NLTK Tokenization\n","['The', 'quick', ',', 'brown', 'fox', '(', 'an', 'exemplary', 'species', ')', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Regex Tokenization\n","['The', 'quick', 'brown', 'fox', 'an', 'exemplary', 'species', 'jumps', 'over', 'the', 'lazy', 'dog', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Spacy Tokenization\n","['The', 'quick', ',', 'brown', 'fox', '(', 'an', 'exemplary', 'species', ')', 'jumps', 'over', 'the', 'lazy', 'dog', '-', 'a', 'classic', 'example', 'of', 'a', 'pangram']\n","Sentence Tokenization\n","['The quick, brown fox (an exemplary species) jumps over the lazy dog - a classic example of a pangram']\n"]}],"source":["import nltk\n","import spacy\n","from nltk.tokenize import word_tokenize, RegexpTokenizer, sent_tokenize\n","\n","# Sample text\n","text = \"The quick, brown fox (an exemplary species) jumps over the lazy dog - a classic example of a pangram\"\n","\n","# White Space Tokenization\n","white_space_tokens = text.split()\n","\n","# NLTK Word Tokenization\n","nltk.download('punkt')\n","nltk_tokens = word_tokenize(text)\n","\n","# Regular Expression Tokenizer\n","regexp_tokenizer = RegexpTokenizer(r'\\w+')\n","regex_tokens = regexp_tokenizer.tokenize(text)\n","\n","# Spacy Tokenization\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","spacy_tokens = [token.text for token in doc]\n","\n","# Sentence Tokenization\n","sentences = sent_tokenize(text)\n","\n","# Collecting all tokenization results\n","tokenization_results = {\n","    \"White Space Tokenization\": white_space_tokens,\n","    \"NLTK Tokenization\": nltk_tokens,\n","    \"Regex Tokenization\": regex_tokens,\n","    \"Spacy Tokenization\": spacy_tokens,\n","    \"Sentence Tokenization\": sentences\n","}\n","\n","for k in tokenization_results.keys():\n","  print(k)\n","  print(tokenization_results[k])"]}]}