{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzr6qUE+N4YNuWoa1eACMV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bag-of-Words"],"metadata":{"id":"wkvLFM5U_Yrb"}},{"cell_type":"markdown","source":["### 1. Load the 20 Newsgroups Dataset"],"metadata":{"id":"Bh-iPF1x_--Z"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","\n","#categories=['sci.space']\n","newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n","\n","# Basic information about the dataset\n","print(\"Target names (Categories):\", newsgroups_data.target_names)\n","print(\"Number of documents:\", len(newsgroups_data.data))\n","print(\"\\nFirst document:\", newsgroups_data.data[0][:200])  # Display first 200 characters of the first document"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDVyAh2a_z1g","executionInfo":{"status":"ok","timestamp":1700424725479,"user_tz":-60,"elapsed":3428,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"4c59fc85-9b4c-4701-b8c6-07e1fe216023"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Target names (Categories): ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n","Number of documents: 18846\n","\n","First document: \n","\n","I am sure some bashers of Pens fans are pretty confused about the lack\n","of any kind of posts about the recent Pens massacre of the Devils. Actually,\n","I am  bit puzzled too and a bit relieved. However,\n"]}]},{"cell_type":"markdown","source":["### 1: Install and Import Necessary Libraries"],"metadata":{"id":"nChL2Khi_dxq"}},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWLHI_nX_Swz","executionInfo":{"status":"ok","timestamp":1700424728342,"user_tz":-60,"elapsed":264,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"adec470c-273a-40f8-c766-cd747ff29a21"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":33}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["### 3. Tokenize the text: example"],"metadata":{"id":"zA-MjXZ2ARYJ"}},{"cell_type":"code","source":["first_doc = newsgroups_data.data[0]\n","tokens = word_tokenize(first_doc)\n","print(\"Tokens of the first document:\\n\", tokens[:20])  # Display first 20 tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwEavbYOAR3B","executionInfo":{"status":"ok","timestamp":1700424730255,"user_tz":-60,"elapsed":239,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"d8168e7a-6dd4-42ec-e7d9-c5af87bab5d6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens of the first document:\n"," ['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about']\n"]}]},{"cell_type":"markdown","source":["### 4. Create a Vocabulary\n","\n","#### 4.1 Using CountVectorizer\n","CountVectorizer will automatically handle tokenization and vocabulary creation. Setting \"lowercase=True\" in CountVectorizer will include the normalization step."],"metadata":{"id":"O31YWS-rAYv5"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Initialize the CountVectorizer with lowercase normalization\n","vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n","vectorizer.fit(newsgroups_data.data)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary Size:\", len(vocab))\n","print(\"Some words in the vocabulary:\\n\", vocab[-20:])  # Display last 20 words in the vocabulary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTX0dJJSAZHQ","executionInfo":{"status":"ok","timestamp":1700424737491,"user_tz":-60,"elapsed":3530,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"766eec10-c405-43cb-f330-56d40f007eee"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 134101\n","Some words in the vocabulary:\n"," ['zzg6c' 'zzi776' 'zzneu' 'zznki' 'zznkj' 'zznkjz' 'zznkzz' 'zznp' 'zzq'\n"," 'zzrk' 'zzs' 'zzvsi' 'zzy_3w' 'zzz' 'zzzoh' 'zzzzzz' 'zzzzzzt' '³ation'\n"," 'ýé' 'ÿhooked']\n"]}]},{"cell_type":"markdown","source":["#### 4.2 Using TfidfVectorizer\n","\n","Use TfidfVectorizer to transform the text data into TF-IDF scores.\n"],"metadata":{"id":"NPTVbpOUBFY8"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize the TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n","tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups_data.data)\n","vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary Size:\", len(vocab))\n","print(\"Some words in the vocabulary:\\n\", vocab[-20:])  # Display last 20 words in the vocabulary\n","\n","print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n","\n","# Display TF-IDF values for the first document\n","first_doc_tfidf = tfidf_matrix[0].toarray()\n","print(\"TF-IDF values for the first document:\\n\", first_doc_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DG-puBIABGvs","executionInfo":{"status":"ok","timestamp":1700424743261,"user_tz":-60,"elapsed":4318,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"0c4e1bca-c2f5-4748-9670-53a7d69a0c32"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 134101\n","Some words in the vocabulary:\n"," ['zzg6c' 'zzi776' 'zzneu' 'zznki' 'zznkj' 'zznkjz' 'zznkzz' 'zznp' 'zzq'\n"," 'zzrk' 'zzs' 'zzvsi' 'zzy_3w' 'zzz' 'zzzoh' 'zzzzzz' 'zzzzzzt' '³ation'\n"," 'ýé' 'ÿhooked']\n","TF-IDF matrix shape: (18846, 134101)\n","TF-IDF values for the first document:\n"," [[0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["### 5. Encoding Documents Using Bag-of-Words\n","\n","#### 5.1 Using CountVectorizer"],"metadata":{"id":"q98RIUwUBQk4"}},{"cell_type":"code","source":["bow_matrix = vectorizer.transform(newsgroups_data.data)\n","print(\"Bag-of-Words matrix shape:\", bow_matrix.shape)  # Shape of the matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ywSasqfjBTs3","executionInfo":{"status":"ok","timestamp":1700405053896,"user_tz":-60,"elapsed":220,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"4e65fbf0-947b-4119-fe9d-c17c8614dc20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag-of-Words matrix shape: (593, 13273)\n"]}]},{"cell_type":"markdown","source":["Display Encoding for a Document"],"metadata":{"id":"R6DoLXnuCcvJ"}},{"cell_type":"code","source":["first_doc_vector = bow_matrix[0].toarray()\n","print(\"encoding for the first document:\\n\", first_doc_vector, \"shape:\", first_doc_vector.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6p-EY6_CdfQ","executionInfo":{"status":"ok","timestamp":1700405867415,"user_tz":-60,"elapsed":226,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"3714ca42-906a-49cc-e199-cbf7cc2283b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encoding for the first document:\n"," [[0 0 0 ... 0 0 0]] shape: (1, 13273)\n","encoding for the first document:\n"," [[0 0 0 ... 0 0 0]]\n"]}]},{"cell_type":"code","source":["first_doc_vector = bow_matrix[0]\n","print(\"Bag-of-Words encoding for the first document (in sparse format):\\n\", first_doc_vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g25b14rzEu8p","executionInfo":{"status":"ok","timestamp":1700405882248,"user_tz":-60,"elapsed":225,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"a0669f49-28a8-4786-e5f0-2cf4d74ac523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag-of-Words encoding for the first document (in sparse format):\n","   (0, 1709)\t1\n","  (0, 1857)\t1\n","  (0, 2421)\t1\n","  (0, 3016)\t1\n","  (0, 3690)\t1\n","  (0, 3761)\t1\n","  (0, 3960)\t1\n","  (0, 4377)\t1\n","  (0, 4561)\t1\n","  (0, 5293)\t2\n","  (0, 5588)\t2\n","  (0, 5945)\t2\n","  (0, 6875)\t1\n","  (0, 6906)\t1\n","  (0, 7177)\t1\n","  (0, 7280)\t1\n","  (0, 7581)\t1\n","  (0, 7605)\t1\n","  (0, 7607)\t2\n","  (0, 7740)\t1\n","  (0, 7788)\t1\n","  (0, 8043)\t1\n","  (0, 8076)\t1\n","  (0, 8162)\t1\n","  (0, 8167)\t3\n","  (0, 8187)\t1\n","  (0, 8361)\t1\n","  (0, 8514)\t1\n","  (0, 8747)\t1\n","  (0, 8753)\t2\n","  (0, 9110)\t1\n","  (0, 9338)\t1\n","  (0, 9375)\t1\n","  (0, 9543)\t1\n","  (0, 9815)\t1\n","  (0, 10072)\t1\n","  (0, 10099)\t1\n","  (0, 10528)\t1\n","  (0, 10605)\t1\n","  (0, 11375)\t1\n","  (0, 11669)\t1\n"]}]},{"cell_type":"markdown","source":["#### 5.1 Using TfidfVectorizer"],"metadata":{"id":"UkqmErnnFdnM"}},{"cell_type":"code","source":["# Fit the model and transform the data\n","tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups_data.data)\n","\n","# Display the shape of the TF-IDF matrix\n","print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)  # Shape of the matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3LKKIttFgdf","executionInfo":{"status":"ok","timestamp":1700422917080,"user_tz":-60,"elapsed":241,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"bb9d824e-bcba-4c4f-b388-2f6f4b5db7fc"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF matrix shape: (593, 13273)\n"]}]},{"cell_type":"markdown","source":["### 6. Use case\n","\n","Split Dataset, Create Bag-of-Words and TF-IDF Models"],"metadata":{"id":"apHVQIzyMTL2"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data, newsgroups_data.target, test_size=0.2, random_state=42)\n","\n","# Bag-of-Words\n","bow_vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n","X_train_bow = bow_vectorizer.fit_transform(X_train)\n","X_test_bow = bow_vectorizer.transform(X_test)\n","\n","# TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)"],"metadata":{"id":"FsQ48r5yMWZE","executionInfo":{"status":"ok","timestamp":1700424823845,"user_tz":-60,"elapsed":7946,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["Train with a MultinomialNB and Evaluate Models"],"metadata":{"id":"kvV_PNmnNvgv"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# Train with Bag-of-Words\n","model_bow = MultinomialNB()\n","model_bow.fit(X_train_bow, y_train)\n","acc_bow = accuracy_score(y_test, model_bow.predict(X_test_bow))\n","\n","# Train with TF-IDF\n","model_tfidf = MultinomialNB()\n","model_tfidf.fit(X_train_tfidf, y_train)\n","acc_tfidf = accuracy_score(y_test, model_tfidf.predict(X_test_tfidf))\n","\n","print(\"Accuracy (Bag-of-Words):\", acc_bow)\n","print(\"Accuracy (TF-IDF):\", acc_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaXQ5yMUMesD","executionInfo":{"status":"ok","timestamp":1700424864430,"user_tz":-60,"elapsed":887,"user":{"displayName":"Maxime Nicolas","userId":"05699040452048060896"}},"outputId":"c930c030-8cd1-40a4-e35b-3188bd30667f"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (Bag-of-Words): 0.6753315649867374\n","Accuracy (TF-IDF): 0.7222811671087533\n"]}]}]}